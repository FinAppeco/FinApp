import os
from pathlib import Path
import pandas as pd
import datetime
from dagster import IOManager, InputContext, check, seven, OutputContext
from dagster.utils.backoff import backoff
from google.cloud import storage
from google.api_core.exceptions import Forbidden, TooManyRequests
from finapp.resources.asset_manager import df_asset_manager
from finapp.resources.utils import upload_parquet_gcs


# mocking path to serialize datetime datatype
from functools import partial
from json import dumps as _dumps

seven.json.dumps = partial(_dumps, sort_keys=True, default=str)



class ParquetGCSIOManager(IOManager):
    """Built-in filesystem IO manager that stores and retrieves values using parquet.

    Args:
        base_dir (Optional[str]): base directory where all the step outputs which use this object
            manager will be stored in.
    """
    def __init__(self, bucket, client=None, prefix="dagster"):
        self.bucket = check.str_param(bucket, "bucket")
        self.client = client or storage.Client()
        self.bucket_obj = self.client.get_bucket(bucket)
        check.invariant(self.bucket_obj.exists())
        self.prefix = check.str_param(prefix, "prefix")

    def _get_path(self, context: OutputContext):
        """Automatically construct filepath."""
        parts = context.get_output_identifier()
        run_id = parts[0]
        task = parts[1].split('.')[0]
        date_execution = datetime.datetime.now().strftime(format='%Y-%m-%d_%H:%M:%S')
        key = "/".join([self.prefix, run_id, date_execution, task]) + '.parquet'
        return key

    def _rm_object(self, key):
        check.str_param(key, "key")
        check.param_invariant(len(key) > 0, "key")

        if self.bucket_obj.blob(key).exists():
            self.bucket_obj.blob(key).delete()

    def _has_object(self, key):
        check.str_param(key, "key")
        check.param_invariant(len(key) > 0, "key")
        blobs = self.client.list_blobs(self.bucket, prefix=key)
        return len(list(blobs)) > 0

    def _uri_for_key(self, key):
        check.str_param(key, "key")
        return "gs://" + self.bucket + "/" + "{key}".format(key=key)

    def handle_output(self, context, obj):
        """Pickle the data and store the object to a file.

        This method omits the AssetMaterialization event so assets generated by it won't be tracked
        by the Asset Catalog.
        """
        key = self._get_path(context)
        context.log.debug(f"Writing GCS object at: {self._uri_for_key(key)}")
        if self._has_object(key):
            context.log.warning(f"Removing existing GCS key: {key}")
            self._rm_object(key)
        # checking if it is a dict or dataframe
        df_asset_manager(context, obj)
        upload_parquet_gcs(context, obj, key, self.bucket_obj)


    def has_output(self, context):
        filepath = self._get_path(context)
        context.log.debug(f"Checking for file at: {filepath}")
        files = list(Path(filepath).glob("*"))
        return len(files) > 0

    def load_input(self, context):
        """Unpickle the file and Load it to a data object."""
        check.inst_param(context, "context", InputContext)

        key = self._get_path(context.upstream_output)
        context.log.debug(f"Loading GCS object from: {self._uri_for_key(key)}")
        bytes_obj = self.bucket_obj.get_blob(key)
        temp_parquet = os.path.join(os.getcwd(), 'temp_parquet_manager', key)
        bytes_obj.download_to_filename(temp_parquet)
        if os.path.exists(temp_parquet):
            try:
                obj = pd.read_parquet(path=temp_parquet, engine='pyarrow')
                if isinstance(obj, pd.DataFrame):
                    return obj
                else:
                    msg = "the file must be a parquet file"
                    raise ValueError(msg)
            except Exception as e:
                raise e

class JsonGCSIOManager(IOManager):
    def __init__(self, bucket, client=None, prefix="dagster"):
        self.bucket = check.str_param(bucket, "bucket")
        self.client = client or storage.Client()
        self.bucket_obj = self.client.get_bucket(bucket)
        check.invariant(self.bucket_obj.exists())
        self.prefix = check.str_param(prefix, "prefix")


    def _get_path(self, context):
        parts = context.get_output_identifier()
        run_id = parts[0]
        output_parts = parts[1:]
        return "/".join([self.prefix, "storage", run_id, "files", *output_parts])

    def _rm_object(self, key):
        check.str_param(key, "key")
        check.param_invariant(len(key) > 0, "key")

        if self.bucket_obj.blob(key).exists():
            self.bucket_obj.blob(key).delete()

    def _has_object(self, key):
        check.str_param(key, "key")
        check.param_invariant(len(key) > 0, "key")
        blobs = self.client.list_blobs(self.bucket, prefix=key)
        return len(list(blobs)) > 0

    def _uri_for_key(self, key):
        check.str_param(key, "key")
        return "gs://" + self.bucket + "/" + "{key}".format(key=key)

    def load_input(self, context):
        """

        """
        key = self._get_path(context.upstream_output)
        context.log.debug(f"Loading GCS object from: {self._uri_for_key(key)}")
        bytes_obj = self.bucket_obj.blob(key).download_as_bytes(client=self.client, raw_download=True)
        json_obj = json.loads(bytes_obj)
        return pd.DataFrame.from_dict(json_obj)

    def handle_output(self, context, obj):
        key = self._get_path(context)
        context.log.debug(f"Writing GCS object at: {self._uri_for_key(key)}")

        if self._has_object(key):
            context.log.warning(f"Removing existing GCS key: {key}")
            self._rm_object(key)
        #checking if it is a dict or dataframe
        if isinstance(obj, dict):
            obj = pd.DataFrame(obj)
        if isinstance(obj, pd.DataFrame):
            context.log.info(f"rows: {len(obj)}")
            try:
                json_obj = obj.to_json(date_format='iso')
                df_asset_manager(context, obj)
                backoff(
                    self.bucket_obj.blob(key).upload_from_string,
                    args=[json_obj, 'application/json', ],
                    retry_on=(TooManyRequests, Forbidden),
                )
            except ValueError as e:
                columns = Counter(obj.columns).most_common()
                # parquet doesn't allow have duplicated name fields in the same file
                columns_duplicated = [column[0] for column in columns if column[1] > 1]
                msg = str(columns_duplicated) + ' These columns are duplicated. One value is possibly repeated in ' \
                                                'the yaml file. Check the section for the solid "{solid_name}"'.format(
                    solid_name=context.solid_def.name)
                logger.error(msg)
                raise ValueError(msg)